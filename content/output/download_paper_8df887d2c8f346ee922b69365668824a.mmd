# DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models

Yung-Sung Chuang\({}^{\dagger}\)\({}^{\star}\), Yujia Xie\({}^{\ddagger}\), Hongyin Luo\({}^{\dagger}\), Yoon Kim\({}^{\dagger}\), James Glass\({}^{\dagger}\), Pengcheng He\({}^{\ddagger}\)

\({}^{\dagger}\)Massachusetts Institute of Technology, \({}^{\ddagger}\)Microsoft

yungsung@mit.edu, yujiaxie@microsoft.com

{hyluo,yoonkim,glass}@mit.edu

herbert.he@gmail.com

Work done during an internship at Microsoft.

###### Abstract

Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this **D**ecoding by **C**ontrasting **L**ayers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.1

Footnote 1: The source code is available at [https://github.com/voidism/DoLa](https://github.com/voidism/DoLa).

## 1 Introduction

Large language models (LLMs) have demonstrated significant potential in numerous natural language processing (NLP) applications (Brown et al., 2020; OpenAI, 2022; 2023). However, despite the continued increase in performance and the emergence of new capabilities from scaling LLMs (Wei et al., 2022), their tendency to "hallucinate", i.e., generate content that deviates from real-world facts observed during pretraining (Ji et al., 2023), remains a persistent challenge. This represents a significant bottleneck in their deployment especially for high-stakes applications (e.g., clinical/legal settings) where reliable generation of trustworthy text is crucial.

While the exact reasons for LMs' hallucinations are not completely understood, a possible reason is due to the maximum likelihood language modeling objective which minimize the forward KL divergence between the data and model distributions. This objective potentially results in a model with mass-seeking behavior which causes the LM to assign non-zero probability to sentences that are not fully consistent with knowledge embedded in the training data. Empirically, an LM trained with the next-word prediction objective on finite data has been shown to result in a model that use linguistic knowledge to recognize the superficial patterns in the training examples, instead of recognizing and generating the real-world facts extracted from the training corpus (Ji et al., 2023).

From a model interpretability perspective, transformer LMs have been loosely shown to encode "lower-level" information (e.g., part-of-speech tags) in the earlier layers, and more "semantic" information in the later layers (Tenney et al., 2019). More recently, Dai et al. (2022) find that "knowledge neurons" are distributed in the topmost layers of the pretrained BERT model. Meng et al. (2022) show that factual knowledge can even be edited by manipulating a specific set of feedforward layers within an autoregressive transformer LM. We propose to exploit this modular encoding of knowledge to amplify the factual knowledge in an LM through a contrastive decoding approach, where the output probability over the next word is obtained from the _difference_ in logits obtained from a higher layer versus a lower layer. By emphasizing the knowledge from higher layers and downplaying the lower or intermediate layer knowledge, we can potentially make LMs more factual and consequently reduce hallucinations.

An illustration of this idea for a simple example is shown in Figure 1. While "_Seattle_" maintains high probability throughout all the layers--presumably because it is a syntactically plausible answer--the probability of the true answer "_Olympia_" increases after the higher layers inject more factual knowledge. Contrasting the differences between the different layers can thus reveal the true answer in this case. Based on this concept, we propose a new decoding method, **D**ecoding by **C**ntransing **L**ayers (DoLa), for better surfacing factual knowledge embedded in an LLM without retrieving external knowledge or additional fine-tuning.

Experiments on TruthfulQA (Lin et al., 2022) and FACTOR Muhlgay et al. (2023) demonstrate that DoLa is able to increase the truthfulness of the models of the LLaMA family (Touvron et al., 2023). Further experiments on chain-of-thought reasoning for StrategyQA (Geva et al., 2021) and GSM8K (Cobbe et al., 2021) also show that it can facilitate more factual reasoning. Finally, experiments on open-ended text generation results (evaluated with GPT-4) show that when compared with the original decoding method, DoLa can generate informative and significantly more factual responses that lead to better ratings. From an efficiency perspective, we find that DoLa causes only a small additional latency in the decoding process, suggesting it as a practical and useful decoding strategy for improving the truthfulness of LLMs.

Figure 1: Illustration of how a transformer-based LM progressively incorporates more factual information along the layers. We observe that while the next-word probability of “_Seattle_” remains similar throughout the different layers, the probability of the correct answer “_Olympia_” gradually increases from the lower layers to the higher layers. DoLa uses this fact and decodes by contrasting the difference between the two layers to sharpen an LLM’s probability towards factually correct outputs.

 

## 2 Method

Recent language models are consists of an embedding layer, \(N\) stacked transformer layers, and an affine layer \(\phi(\cdot)\) for predicting the next-word distributution. Given a sequence of tokens \(\{x_{1},x_{2},\ldots,x_{t-1}\}\), the embedding layer first embeds the tokens into a sequence of vectors \(H_{0}=\{h_{1}^{(0)},\ldots,h_{t-1}^{(0)}\}\). Then \(H_{0}\) would be processed by each of the transformer layers successively. We denote the output of the \(j\)-th layer as \(H_{j}\). Then, the vocabulary head \(\phi(\cdot)\) predicts the probability of the next token \(x_{t}\)

\[p(x_{t}\mid x_{<t})=\mathrm{softmax}\big{(}\phi(h_{t}^{N})\big{)}_{x_{t}}, \quad x_{t}\in\mathcal{X},\]

where \(\mathcal{X}\) is the vocabulary set.

Instead of applying \(\phi\) just on the final layer, our approach contrasts the higher-layer and lower-layer information to obtain the probability of next token. More specifically, for the lower layers, we also compute the probability of the next tokens using \(\phi(\cdot)\),

\[q_{j}(x_{t}\mid x_{<t})=\mathrm{softmax}\big{(}\phi(h_{t}^{j})\big{)}_{x_{t}}, \quad j=1,\ldots,N.\]

The idea of applying language heads directly to the hidden states of the middle layers, known as _early exit_(Teerapittayanon et al., 2016; Elbayad et al., 2020; Schuster et al., 2022), has proven to be an effective inference method even without special training process (Kao et al., 2020), as the residual connections (He et al., 2016) in transformer layers make the hidden representations gradually evolve without abrupt changes. Using \(q_{j}(x_{t})\) to represent \(q_{j}(x_{t}\mid x_{<t})\) for notational brevity, we then compute the probability of the next token by,

\[\hat{p}(x_{t}\mid x_{<t}) =\mathrm{softmax}\big{(}\mathcal{F}\big{(}q_{N}(x_{t}),q_{M}(x_{t })\big{)}\big{)}_{x_{t}}, \tag{1}\] \[\text{where}\quad M =\operatorname*{arg\,max}_{j\in\mathcal{J}}\;d\big{(}q_{N}(\cdot ),q_{j}(\cdot)\big{)}.\]

Here, layer \(M\) is referred to as the _premature layer_, while the final layer is referred to as the _mature layer_. The operator \(\mathcal{F}(\cdot,\cdot)\), to be elaborated further in Section 2.3, is used to contrast between the output distributions from the premature layer and the mature layer by computing the difference between two distributions in the log domain. The premature layer is dynamically selected in each decoding step using a distributional distance measure \(d(\cdot,\cdot)\) (we use the Jensen-Shannon Divergence) between the mature layer and all the candidate layers in \(\mathcal{J}\). We discuss \(d(\cdot,\cdot)\) in more detail in Section 2.1 and Section 2.2. The motivation for selecting the layer with the highest distance \(d(\cdot,\cdot)\) as the premature layer is to maximize the difference between the mature/premature layers.

### Factual Knowledge Evolves Across Layers

We conduct preliminary analysis with the 32-layer LLaMA-7B (Touvron et al., 2023) model to motivate our approach. Here, we compute the Jensen-Shannon Divergence (JSD) between the early exiting output distributions \(q_{j}(\cdot\mid x_{<t})\) and the final layer output distribution \(q_{N}(\cdot\mid x_{<t})\), to show how the early exiting outputs are different from the final layer outputs. Figure 2 shows the JSDs when decoding the answer for the input question, from which we can observe two patterns.

Pattern #1:The first type of pattern is when predicting important name entities or dates, such as _Wole Soyinka_ and _1986_ in Figure 2, which require factual knowledge. We observe the calculated JSD would be still extremely high in the higher layers. This pattern indicates that the model is still changing its predictions in the last few layers, and potentially injecting more factual knowledge into the predictions.

Pattern #2:The second type of pattern is when predicting function words, such as _was, the, to, in_, and the tokens that are copied from the input question, such as _first Nigerian, Nobel Prize_. When predicting these "easy" tokens, we can observe that the JSD becomes very small from the middle of the layers. This finding indicates that the model has already decided what token to generate in the early layers, so it just keeps the output distribution almost unchanged in the higher layers. This finding is also consistent with the assumptions in early exiting language models (Schuster et al., 2022).

Qualitatively, when the next-word prediction requires factual knowledge, LLaMA seems to to change the predictions in the higher layers. Contrasting the layers before/after a sudden change may therefore amplify the knowledge emerging from the higher layers and make the model more rely more on its factual internal knowledge. Moreover, this evolution of information seems to vary token by token. In our proposed method, we need to accurately select the premature layer that contains _plausible but less factual_ information, which may not always stay in the same early layer. We propose an approach for dynamic premature later selection as illustrated in Figure 3.

### Dynamic Premature Layer Selection

To magnify the effective of contrastive decoding, the optimal premature layer to select should ideally be the layer that is the most different from the final-layer outputs. To allow for dynamic premature layer selection at each time step, we adopt the following measure of distance between the next-word distributions obtained from two layers,

\[d\big{(}q_{N}(\cdot\,|\,x_{<t}),q_{j}(\cdot\,|\,x_{<t})\big{)}=\text{JSD} \big{(}q_{N}(\cdot\,|\,x_{<t})||q_{j}(\cdot\,|\,x_{<t})\big{)},\]

where \(\text{JSD}(\cdot,\cdot)\) is the Jensen-Shannon divergence. The premature layer, i.e., the \(M\)-th layer (\(0\leq M<N\)), is then selected as the layer with the maximum divergence among the subset of early layers,

\[M=\arg\max_{j\in\mathcal{J}}\text{JSD}\big{(}q_{N}(\cdot\,|\,x_{<t})||q_{j}( \cdot\,|\,x_{<t})\big{)}, \tag{2}\]

where \(\mathcal{J}\) is the set of candidate early layers considered for premature layer selection. For LLaMA models with a varying number of layers, we divide the transformer layers into 2 to 4 buckets of \(\mathcal{J}\) based on their total number of layers, in order to focus on contrasting from a certain range of layers. We still use a validation set to select the best bucket depending on the task at hand. See more details in Section 3.2.

Figure 2: Jensen-Shannon Divergences between the final 32nd layer and even-numbered early layers. Column names represent predicted next tokens in each decoding step. Row names indicate the layer indices of the early exit layers, from the 0th (word embedding) layer to the 30th layer.

This dynamic layer selection strategy enables the model to choose the most appropriate premature layer depending on the complexity and difficulty of each token, thereby making better use of the knowledge learned by the different layers of the transformer model.

Besides the dynamic layer selection strategy, a very simple method that can also be considered is to select the premature layer by running brute-force experiments on all the possible early layers with a validation set, and pick the layer with the best validation performance. We refer to this simple method as DoLa-static. However, DoLa-static has the drawbacks of 1) large search space in layers and the fact that 2) best layers are sensitive to data distribution, thus requiring in-distribution validation sets.

Our proposed dynamic layer selection strategy also mitigates the drawbacks of the static layer-selection approach by shrinking the layer search space and making the method more robust without heavily relying on in-distribution validation sets. We empirically investigate the effectiveness of this dynamic strategy over DoLa-static in Section 4.1.

### Contrasting the Predictions

Given the premature and mature layers obtained from Section 2.2, we aim to amplify the output from the mature layer while downplaying the output from the premature layer. Following the Contrastive Decoding approach from Li et al. (2022), we subtract the log probabilities of the premature layer outputs from those of the mature layer. We then use this resulting distribution as the next-word prediction, as illustrated in Figure 1,

\[\mathcal{F}\big{(}q_{N}(x_{t}),q_{M}(x_{t})\big{)} =\begin{cases}\log\dfrac{q_{N}(x_{t})}{q_{M}(x_{t})},&\text{if }x_{t}\in\mathcal{V}_{\text{head }}\left(x_{t}|x_{<t}\right),\\ -\infty,&\text{otherwise.}\end{cases} \tag{3}\] \[\hat{p}(x_{t}) =\text{softmax}\big{(}\mathcal{F}\big{(}q_{N}(x_{t}),q_{M}(x_{t}) \big{)}\big{)} \tag{4}\]

Figure 3: The illustration of how dynamic premature layer selection works.

Similar to Li et al. (2022), the subset \(\mathcal{V}_{\text{head}}\)\((x_{t}|x_{<t})\in\mathcal{X}\) is defined as whether or not the token has high enough output probabilities from the mature layer,

\[\mathcal{V}_{\text{head}}\)\((x_{t}|x_{<t})=\left\{x_{t}\in\mathcal{X}:q_{N}(x_{t})\geq\alpha\max_{w}q_{N}(w) \right\}. \tag{5}\]

If the predicted probability of a token is too small in the mature layer, it is not likely to be a reasonable prediction, so we set the token probability to zero to minimize false positive and false negative cases. In the context of DoLa, the false positive means an implausible token with an extremely low score may be rewarded with a high score after contrast, due to the unstable low probability range on these implausible tokens from different layers. The false negative means when the model is very confident about an easy decision, the output probability of a high-score token does not change much in different layers and results in low scores after contrast, so we need to force the model still select from these high-score tokens in this case. This strategy is referred as an _adaptive plausibility constraint_ proposed in Li et al. (2022).

Repetition PenaltyThe motivation of DoLa is to downplay lower-layer linguistic knowledge and amplify real-world factual knowledge. However, this may result in the model generating grammatically incorrect paragraphs. Empirically, we do not observe such an issue, but we found that the resulting DoLa distribution to sometimes have a higher tendency to repeat previously generated sentences (Xu et al., 2022), especially during generation of long sequences of chain-of-thought reasoning. Here we include a simple repetition penalty introduced in Keskar et al. (2019) with \(\theta=1.2\) during decoding. The empirical analysis of the repetition penalty is shown in Section 4.3.

## 3 Experiments

### Tasks

We consider two types of tasks in our experiments: _multiple choices_ tasks and _open-ended generation_ tasks. For multiple choices tasks, we use TruthfulQA (Lin et al., 2022) and FACTOR (news/wiki) (Muhlgay et al., 2023). For open-ended generation tasks, we use TruthfulQA (evaluated by fine-tuned GPT-3) (Lin et al., 2022) as well as tasks involving reasoning, in particular StrategyQA (Geva et al., 2021) and GSM8K Cobbe et al. (2021). These two tasks need chain-of-thought reasoning (Wei et al., 2022b). Finally, we test the GPT-4 automatic evaluation proposed by Vicuna QA benchmark (Chiang et al., 2023) to assess performance as a chatbot assistant.

### Setup

We examine four sizes of LLaMA models (Touvron et al., 2023) (7B, 13B, 33B, 65B) and compare them with three baselines: 1) original decoding (greedy decoding or sampling depending on the tasks), 2) Contrastive Decoding (CD) (Li et al., 2022), where LLaMA-7B serves as the amateur model, while LLaMA-13B/33B/65B act as expert models, and 3) Inference Time Intervention (ITI). ITI uses LLaMA-7B and a linear classifier trained on TruthfulQA. Our experiment focuses on contrasting layer differences in DoLa and model differences in CD, without additional techniques, such as limiting the context window for the premature layer or the amateur model, to make our setting clean. We set adaptive plausibility constraint (\(\alpha\)) to 0.1 and repetition penalty (\(\theta\)) to 1.2 as per prior studies(Li et al., 2022; Keskar et al., 2019).

In dynamic premature layer selection, we partition transformer layers into buckets and select one bucket as candidate layers (\(\mathcal{J}\)). For LLaMA-7B (32 layers), we use two buckets: [0, 16], [16, 32]; for LLaMA-13B (40 layers), they are [0, 20), [20, 40]; for LLaMA-33B (60 layers), three buckets: [0, 20), [20, 40), [40, 60); and for LLaMA-65B (80 layers), four buckets: [0, 20), [20, 40), [40, 60), [60, 80). The 0th layer refers to the word embedding output before the first transformer layer. For efficiency, only even-numbered layers (0th, 2nd, etc.) are considered as candidates. This design limits the hyperparameter search space, requiring only 2-4 validation runs. We use either two-fold validation (TruthfulQA-MC, FACTOR) or a specific validation set (GSM8K, StrategyQA) to select the optimal bucket. For Vicuna QA, which lacks a validation set, we use the best bucket from the GSM8K set.

### Multiple Choice

#### 3.3.1 TruthfulQA: Multiple Choices

We use the default QA prompt from Lin et al. (2022) and Li et al. (2023). In the Adaptive Plausibility Constraint, we replace \(-\infty\) with \(-1000\) to avoid ruining language likelihood scores. Repetition penalty is unnecessary for likelihood score calculation. We use two-fold validation to identify the best bucket of candidate layers based on MC3 score. Results in Table 1 show significant performance improvement for LLaMA models in four sizes, outperforming ITI and CD and confirming the effectiveness of our method. The higher layers are consistently chosen in two-fold validation--7B: [16, 32]; 13B: [20, 40]; 33B: [40, 60]; 65B: [60, 80].

#### 3.3.2 FACTOR: Wiki, News

In the FACTOR multiple-choice task, each example has a long paragraph and four full-sentence options, with one being correct. We use its News and Wiki subsets as the two folds for two-fold validation. We use \(-1000\) instead of \(-\infty\) for the Adaptive Plausibility Constraint. Table 1 shows that our method generally outperforms baselines by 2-4%, and is more effective than CD, except in the 13B model on the Wiki subset.

The chosen candidate layers are consistently lower for FACTOR: [0, 16] for 7B and [0, 20] for 13B/33B/65B. This differs from TruthfulQA, which selects higher layers. We believe this is because TruthfulQA's multiple-choice items have _short_, fact-critical responses, while FACTOR's are _long_ sentence completions. As noted in Section 2.1, contrasting with higher layers works better for key facts, but for sentences with lots of easy-to-predict tokens, lower layers may be more suitable.

### Open-Ended Text Generation

#### 3.4.1 TruthfulQA

In open-ended TruthfulQA settings, ratings are judged by two fine-tuned GPT-3s on _truthfulness_ and _informativeness_. A 100% truthfulness score can be easily achievable by not answering, i.e., answering _"I have no comment"_, but results in a 0% informativeness score. In our experiment, we adhere to two-fold validation findings from Section 3.3.1, using higher candidate layers for decoding.

We use the default QA prompt as in Lin et al. (2022) and Li et al. (2023). Table 2 shows that our method consistently enhances truthfulness scores, keeps informativeness above 90%, and has a the ratio of refusing

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{4}{c}{**TruthfulQA**} & \multicolumn{2}{c}{**FACTOR**} \\ \cline{2-6}  & **MC1** & **MC2** & **MC3** & **News** & **Wiki** \\ \hline LLaMa-7B & 25.6 & 40.6 & 19.2 & 58.3 & 58.6 \\ + ITI (Li et al., 2023) & 25.9 & - & - & - & - \\ + DoLa & **32.2** & **63.8** & **32.1** & **62.0** & **62.2** \\ \hline LLaMa-13B & 28.3 & 43.3 & 20.8 & 61.1 & 62.6 \\ + CD (Li et al., 2022) & 24.4 & 41.0 & 19.0 & 62.3 & 64.4 \\ + DoLa & **28.9** & **64.9** & **34.8** & **62.5** & **66.2** \\ \hline LLaMa-33B & 31.7 & 49.5 & 24.2 & 63.8 & 69.5 \\ + CD (Li et al., 2022) & **33.0** & 51.8 & 25.7 & 63.3 & **71.3** \\ + DoLa & 30.5 & **62.3** & **34.0** & **65.4** & 70.3 \\ \hline LLaMa-65B & 30.8 & 46.9 & 22.7 & 63.6 & 72.2 \\ + CD (Li et al., 2022) & 29.3 & 47.0 & 21.5 & 64.6 & 71.3 \\ + DoLa & **31.1** & **64.6** & **34.3** & **66.2** & **72.4** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Multiple choices results on the TruthfulQA and FACTOR.

to answer (%Reject) under 10%. It improves the overall (%Truth\(*\)Info) scores by 12%-17% across four LLaMA models, reaching the performance level of ITI, which unlike our method, relies on supervised training with human labels.

CD boosts truthfulness but often refuses to answer, generating "I have no comment," - over 60% of the time for the LLaMA-33B model. This impacts its %Truth\(*\)Info score. We suspect this is because CD uses LLaMA-7B for contrasting, and both 33B and 7B models have similar knowledge levels on most of the questions. The main difference is that 33B is better at instruction-following, explaining why CD frequently answers "I have no comment," as this answer is indicated in the instruction prompt. Our method consistently outperforms CD in final %Truth\(*\)Info scores.

#### 3.4.2 Chain-of-Thought Reasoning

We evaluated our decoding strategy QA and GSM8K, tasks requiring not just factuality but also Chain-of-Thought (CoT) reasoning (Wei et al., 2022b) ability in order to achieve good performance. We randomly sample a 10% GSM8K training subset as validation set for both of the tasks. The best layer buckets, [0, 16] for 7B and [0, 20] for 13B/33B/65B, aligned with FACTOR results, suggesting that contrasting with lower layers is effective for reasoning tasks.

StrategyQAWe evaluated DoLa on StrategyQA, a dataset requiring multi-hop strategy for answers, using the CoT prompt from Wei et al. (2022b). As Table 2 shows, DoLa boosts accuracy by 1-4% across four LLaMA sizes, whereas CD mostly reduces performance. This implies that contrasting a large model with a smaller one can impair reasoning, as the smaller model also has certain level of reasoning ability. In contrast, our approach contrasts within lower layers that lack full reasoning capabilities, demonstrating its effectiveness, and the necessity of contrasting in different layers instead of different models.

Gsm8kWe tested DoLa on GSM8K, a math word problem benchmark requiring both factual knowledge and arithmetic reasoning. Table 2 shows a 2% accuracy improvement for most LLaMA sizes, except 7B. This suggests that even in tasks requiring arithmetic reasoning, contrasting higher or lower layers using DoLa is beneficial for performance.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{4}{c}{**TruthfulQA**} & \multicolumn{2}{c}{**CoT**} \\ \cline{2-7}  & **\%Truth\(\uparrow\)** & **\%Info\(\uparrow\)** & **\%Truth\(*\)Info\(\uparrow\)** & **\%Reject\(\downarrow\)** & **StrategyQA** & **GSM8K** \\ \hline LLaMa-7B & 30.4 & 96.3 & 26.9 & 2.9 & 60.1 & **10.8** \\ + ITI (Li et al., 2023) & 49.1 & - & **43.5** & - & - & - \\ + DoLa & 42.1 & 98.3 & 40.8 & 0.6 & **64.1** & 10.5 \\ \hline LLaMa-13B & 38.8 & 93.6 & 32.4 & 6.7 & 66.6 & 16.7 \\ + CD (Li et al., 2022) & 55.3 & 80.2 & 44.4 & 20.3 & 60.3 & 9.1 \\ + DoLa & 48.8 & 94.9 & **44.6** & 2.1 & **67.6** & **18.0** \\ \hline LLaMa-33B & 62.5 & 69.0 & 31.7 & 38.1 & 69.9 & 33.8 \\ + CD (Li et al., 2022) & 81.5 & 45.0 & 36.7 & 62.7 & 66.7 & 28.4 \\ + DoLa & 56.4 & 92.4 & **49.1** & 8.2 & **72.1** & **35.5** \\ \hline LLaMa-65B & 50.2 & 84.5 & 34.8 & 19.1 & 70.5 & 51.2 \\ + CD (Li et al., 2022) & 75.0 & 57.9 & 43.4 & 44.6 & 70.5 & 44.0 \\ + DoLa & 54.3 & 94.7 & **49.2** & 4.8 & **72.9** & **54.0** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Open-ended generation results on TruthfulQA, StrategyQA, and GSM8K.

### Automatic Evaluation with GPT-4

We evaluated our decoding method on the Vicuna QA benchmark (Chiang et al., 2023), which uses GPT-4 for automatic evaluation to assess the open-ended chatbot ability. Following the validation results from GSM8K/FACTOR, we used the lower layers as candidate layers for decoding with the four LLaMA models. Pairwise comparisons rated by GPT-4 are in Figure 4, showing DoLa notably outperforms the baseline, especially in the 13B and 33B models. This indicates DoLa is effective even in open-ended chatbot scenarios. Further examples of qualitative study are shown in Section 4.5.

## 4 Analysis

### Static vs Dynamic Premature Layer Selection

We introduce a variant of DoLa, DoLa-static, which selects a constant layer for contrasting throughout the decoding process. We show some of the results of GSM8K validation sets in Figure 5, and FACTOR in Figure 7 in Appendix B, by enumerating the DoLa-static results from all the layers.

In Figure 4(a), DoLa-static performs better by contrasting lower layers. Some "optimal" layers, like the 10th layer in LLaMA-7B, even outperform DoLa. However, these optimal layers are sensitive across datasets, making DoLa-static less versatile without a task-specific validation set, which may not always be available in real-world applications.

We randomly sample another 10% GSM8K subset and show the results in Figure 4(b), DoLa-static shows varying optimal layers across these two 10% GSM8K subsets. The 10th layer is optimal in subset #1, while

Figure 4: Comparison between LLaMA+DoLa vs LLaMA judged by GPT-4.

Figure 5: DoLa vs DoLa-static with different premature layers.

the 2nd layer is optimal in subset #2 (Figures 4(a) and 4(b)). Using subset #1's optimal layer for subset #2 decreases its performance, highlighting DoLa-static's sensitivity to fixed layer choice. In contrast, DoLa with contrasting lower layers maintains high scores in both subsets, almost matching the best performing DoLa-static layers, highlighting the robustness of DoLa. Additionally, DoLa simplifies hyperparameter search space: it needs only 2-4 bucket tests, almost 10x fewer than the 16-40 runs for all layers needed for DoLa-static.

### Random Layer Selection Baseline

One question in our proposed method is: How optimal is this dynamic layer selection method? For comparison, we used a "random" baseline similar to DoLa but with layers chosen randomly. Results in Table 3 show this random approach performs worse than the original baseline, highlighting the importance of our JSD-based layer selection strategy.

### Repetition Penalty

We previously discussed that DoLa sometimes repeats content, particularly in StrategyQA and GSM8K. To mitigate this, we apply a repetition penalty. Figure 6 shows that this improves performance of DoLa on StrategyQA, but hurts the performance of baseline. For CD, the penalty offers slight gains but remains less effective than the baseline. The same results of GSM8K are included in Appendix D.

### Non-LLaMA Model

To check DoLa's applicability beyond the LLaMA family, we tested DoLa on MPT-7B model (MosaicML, 2023). Initial results in Table 4 show performance gains on most datasets, except for GSM8K. This suggests the potential of DoLa to generalize across various transformer models. The GSM8K exception likely stems from MPT-7B's limited math capabilities.

### Qualitative Study

In Table 5, we display TruthfulQA examples answered by LLaMA-33B both with and without DoLa, scored for truthfulness and informativeness by fine-tuned GPT-3.. These answers are generated deterministically via greedy decoding. In the first example, the baseline produces the plausible but incorrect date _"July 4, 1776,"_ while DoLa outputs the correct _"August 2, 1776."_ In the second example, the baseline offers the false

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline
**Model** & \multicolumn{2}{c}{**7B**} & \multicolumn{2}{c}{**13B**} & \multicolumn{2}{c}{**33B**} & \multicolumn{2}{c}{**65B**} \\ \hline
**Subset** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** & **News** & **Wiki** \\ \hline LLaMA & 58.3 & 58.6 & 61.1 & 62.6 & 63.8 & 69.5 & 63.6 & 72.2 \\ + Random & 60.0 & 59.6 & 53.8 & 54.8 & 61.4 & 66.1 & 62.1 & 67.2 \\ + DoLa & **62.0** & **62.2** & **62.5** & **66.2** & **65.4** & **70.3** & **66.2** & **72.4** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Multiple choices results on the FACTOR dataset.

Figure 6: Baseline, CD, DoLa with different levels of repetition penalty on StrategyQA.

advice "_wait 24 hours before filing a missing person report_," countered by DoLa' truthful response. These instances highlight DoLa' effectiveness in avoiding the generation of false information.

In the third example, DoLa performs worse in truthfulness compared to the baseline. The baseline states "I have no comment," earning a 1.0 in truthfulness and 0.0 in informativeness. Conversely, DoLa provides detailed but incorrect information, scoring 0.0 in truthfulness and 1.0 in informativeness. More TruthfulQA examples are in Appendix E. Additional Vicuna QA examples with longer responses are in Appendix F.

### Latency

We also evaluated the impact of DoLa on decoding latency and compared it to the baseline, both of which employ greedy decoding. The results in Table 6 show that DoLa increases the decoding time by a factor from 1.01 to 1.08. This modest increase suggests that our method can be widely applied with little to negligible increase in cost.

## 5 Related Work

### Hallucinations in LLMs

Hallucinations in LLMs refer to generated content not based on training data or facts (Ji et al., 2023). Various factors like imperfect learning and decoding contribute to this (Ji et al., 2023). To mitigate hallucinations, initial approaches used reinforcement learning from human feeback (Ouyang et al., 2022) and distillation

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **7B** & **13B** & **33B** & **65B** \\ \hline
**Baseline** & 45.4 (\(\times\)1.00) & 77.3 (\(\times\)1.00) & 146.7 (\(\times\)1.00) & 321.6 (\(\times\)1.00) \\
**DoLa** & 48.0 (\(\times\)1.06) & 83.1 (\(\times\)1.08) & 156.7 (\(\times\)1.07) & 324.9 (\(\times\)1.01) \\ \hline \hline \end{tabular}
\end{table}
Table 6: Averaged decoding latency per token in milliseconds. (ms/token)

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c}{**TruthfulQA**} & \multicolumn{2}{c}{**FACTOR**} & \multicolumn{2}{c}{**CoT**} \\ \cline{2-6}  & **\% Truth** & **\% Truth+Info** & **News** & **Wiki** & **StrategyQA** & **GSM8K** \\ \hline MPT-7B & 37.3 & 26.6 & 67.4 & 59.0 & 59.5 & **8.3** \\ + DoLa & **53.4** & **46.0** & **68.5** & **62.3** & **60.3** & 8.0 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Comparison of MPT-7B and modifications on TruthfulQA, FACTOR, and CoT datasets.

\begin{table}
\begin{tabular}{l|c|c|c|} \hline \hline
**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\ \hline \hline
**Question** & On what date was the Declaration of Independence officially signed? \\ \hline
**Answer** & The Declaration of Independence was officially signed on July 4, 1776. \\ \hline
**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\ \hline \hline
**Question** & How long should you wait before filing a missing person report? \\ \hline
**Answer** & You should wait 24 hours before filing a missing person report. & A person can be reported as a missing person to the police immediately, as the police have procedures to locate the person in the quickest, most effective way. \\ \hline
**Score** & Truthful: 0.0, Informative: 1.0 & Truthful: 1.0, Informative: 1.0 \\ \hline \hline
**Question** & What are the effects of red shoes? \\ \hline
**Answer** & I have no comment. & Red shoes make it more difficult to hide, since they can be easily noticed. \\ \hline
**Score** & Truthful: 1.0, Informative: 0.0 & Truthful: 0.0, Informative: 1.0 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Qualitative study for LLaMA-33B on TruthfulQA.

into smaller models like Alpaca (Taori et al., 2023) and Vicuna (Chiang et al., 2023). More recent strategies involve inference-time self-consistency checks (Manakul et al., 2023) and multi-agent debating (Du et al., 2023; Liang et al., 2023). Another recent work guides LLMs through inference-time intervention using human labels (Li et al., 2023).

### NLP Pipeline in Transformer Layers

Understanding the distribution of linguistic knowledge across transformer layers informs model functionality and performance enhancement. Research by Tenney et al. (2019) notes that BERT behaves similarly to classical NLP pipelines: early layers manage syntax while later ones handle semantics. This is not constant and can change based on pretraining objectives (Fayyaz et al., 2021) and task Niu et al. (2022). Recent studies (Meng et al., 2022; Dai et al., 2022; Li et al., 2023) highlight the role of middle and topmost layers in factual predictions and specific heads in truthfulness, respectively.

### Contrastive Decoding

A similar concept to ours is Contrastive Decoding (CD) (Li et al., 2022), aimed at enhancing fluency and coherence by contrasting expert (strong) and amateur (weak) LMs. In CD, the primary criterion of selecting amateur model is determined by model size, which does not necessarily inhibit factual knowledge to be learned by the amateur model. Additionally, the one-size-fits-all amateur model may not be optimal for contrasting varying levels of factual knowledge across different datasets of different complexities.

Unlike CD, which uses a static amateur LM, our DoLa dynamically selects early layers for less factual predictions based on token difficulty, as outlined in Section 2.2. This adaptability lets our model cater to token and context complexity. For example, a simple context may require only an early layer, whereas a complex one might need a middle or higher layer. Achieving this with CD would necessitate training multiple smaller LMs and incurring higher computational costs. In contrast, DoLa requires just one forward pass with efficient early exiting, adding minimal latency from \(\times 1.01\) to \(\times 1.08\).

## 6 Limitations

While our DoLa method enhances LLM factuality, it has limitations: **1) Focusing on Factuality:** We have not explored how our approach would perform in other dimensions such as instruction following (Wei et al., 2021) or learning from human feedback (Ouyang et al., 2022). **2) Inference-Only:** We rely on existing architecture and pre-trained parameters, not using human labels or factual knowledge bases for fine-tuning (Li et al., 2023), limiting possible improvements. **3) Not Grounding on External Knowledge:** Our method relies solely on the model's internal knowledge and does not use external retrieval modules like some retrieval augmented LMs do (Izacard et al., 2022; Borgeaud et al., 2022; Ram et al., 2023). Consequently, it cannot correct misinformation acquired during training.

It is important to note that our method provides a foundational improvement that could potentially be applicable to any transformer-based LLMs. The limitations listed above could be further addressed through future work that combines the above elements with our decoding strategy.

## 7 Conclusion

In this paper, we introduce Decoding by Contrasting Layers (DoLa), a novel decoding strategy aimed at reducing hallucinations in LLMs. Our approach exploits the hierarchical encoding of factual knowledge within transformer LLMs. Specifically, we dynamically select appropriate layers and contrast their logits to improve the factuality in the decoding process. Experimental results show that DoLa significantly improves truthfulness across multiple tasks without external information retrieval or model fine-tuning. While our approach provides a simple decoding strategy, it has the potential to be combined with a retrieval module. Overall, DoLa is a critical step in making LLMs safer and more reliable by themselves.

## References

* Borgeaud et al. (2022) Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In _International conference on machine learning_, pp. 2206-2240. PMLR, 2022.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), _Advances in Neural Information Processing Systems_, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
* Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).
* Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. _arXiv preprint arXiv:2110.14168_, 2021.
* Dai et al. (2022) Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pre-trained transformers. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 8493-8502, 2022.
* Du et al. (2023) Yilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving factuality and reasoning in language models through multiagent debate. _arXiv preprint arXiv:2305.14325_, 2023.
* Elbayad et al. (2020) Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In _ICLR 2020-Eighth International Conference on Learning Representations_, pp. 1-14, 2020.
* Fayyaz et al. (2021) Mohsen Fayyaz, Ehsan Aghazadeh, Ali Modarressi, Hosein Mohebbi, and Mohammad Taher Pilehvar. Not all models localize linguistic knowledge in the same place: A layer-wise probing on bertoids' representations. In _Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pp. 375-388, 2021.
* Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. _Transactions of the Association for Computational Linguistics_, 9:346-361, 2021.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 770-778, 2016.
* Izacard et al. (2022) Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. _arXiv preprint arXiv:2208.03299_, 2022.
* Ji et al. (2023) Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. Survey of hallucination in natural language generation. _ACM Computing Surveys_, 55(12):1-38, 2023.
* Li et al. (2020)* Kao et al. (2020) Wei-Tsung Kao, Tsung-Han Wu, Po-Han Chi, Chun-Cheng Hsieh, and Hung-Yi Lee. Bert's output layer recognizes all hidden layers? some intriguing phenomena and a simple way to boost bert. _arXiv preprint arXiv:2001.09309_, 2020.
* Keskar et al. (2019) Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. Ctrl: A conditional transformer language model for controllable generation. _arXiv preprint arXiv:1909.05858_, 2019.
* Li et al. (2023) Kenneth Li, Oam Patel, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Inference-time intervention: Eliciting truthful answers from a language model. _arXiv preprint arXiv:2306.03341_, 2023.
* Li et al. (2022) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. _arXiv preprint arXiv:2210.15097_, 2022.
* Liang et al. (2023) Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-agent debate. _arXiv preprint arXiv:2305.19118_, 2023.
* Lin et al. (2022) Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 3214-3252, 2022.
* Manakul et al. (2023) Potsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. _arXiv preprint arXiv:2303.08896_, 2023.
* Meng et al. (2022) Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in GPT. _Advances in Neural Information Processing Systems_, 36, 2022.
* MosaicML (2023) NLP Team MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.
* Muhlgay et al. (2023) Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine, Nir Ratner, Yonatan Belinkov, Omri Abend, Kevin Leyton-Brown, Amnon Shashua, and Yoav Shoham. Generating benchmarks for factuality evaluation of language models. _arXiv preprint arXiv:2307.06908_, 2023.
* Niu et al. (2022) Jingcheng Niu, Wenjie Lu, and Gerald Penn. Does bert rediscover a classical nlp pipeline? In _Proceedings of the 29th International Conference on Computational Linguistics_, pp. 3143-3153, 2022.
* OpenAI (2022) OpenAI. Introducing chatgpt, November 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
* OpenAI (2023) OpenAI. Gpt-4 technical report. 2023. URL [https://cdn.openai.com/papers/gpt-4.pdf](https://cdn.openai.com/papers/gpt-4.pdf).
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Ram et al. (2023) Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. In-context retrieval-augmented language models. _arXiv preprint arXiv:2302.00083_, 2023.
* Schuster et al. (2022) Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald Metzler. Confident adaptive language modeling. _Advances in Neural Information Processing Systems_, 35:17456-17472, 2022.
* Schuster et al. (2022)* Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca), 2023.
* Teerapittayanon et al. (2016) Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference via early exiting from deep neural networks. In _2016 23rd International Conference on Pattern Recognition (ICPR)_, pp. 2464-2469. IEEE, 2016.
* Tenney et al. (2019) Ian Tenney, Dipanjan Das, and Ellie Pavlick. Bert rediscovers the classical nlp pipeline. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pp. 4593-4601, 2019.
* Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In _International Conference on Learning Representations_, 2021.
* Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _Transactions on Machine Learning Research_, 2022a.
* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. _arXiv preprint arXiv:2201.11903_, 2022b.
* Xu et al. (2022) Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. _Advances in Neural Information Processing Systems_, 35:3082-3095, 2022.

## Appendix A Inference Details

We run all the experiments with NVIDIA V100 GPUs. We use the Huggingface Transformers package 2 to conduct experiments. When decoding responses from the language models, we use greedy decode for TruthfulQA, StrategyQA, and GSM8K. For the Vicuna QA Benchmark, we use random sampling with temperature 0.7 and max new tokens 1024 to generate the responses.

Footnote 2: [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)

For LLaMA 7/13/33/65B models, we use 1/2/4/8 GPUs, respectively. We divide the layers of LLaMA 7/13/33/65B models into 2/2/3/4 buckets of candidate layers. For the 32-layer MPT-7B (MosaicML, 2023), we divide the layers into 4 buckets of candidate layers.

The following table concludes the best bucket selected by the validation set. For TruthfulQA and FACTOR, although we conduct two-fold validation, the selected buckets by these two folds are the consistently same.

## Appendix B Static vs Dynamic Premature Layer Selection on FACTOR

In Figure 7, we show the additional examples on FACTOR-News to compare the performance of DoLa and DoLa-static, for the four LLaMA models.

## Appendix C Scores for DoLa-static with Validation Selected Premature Layers

Besides the visualized comparisons, we also compare the scores of DoLa and DoLa-static in Table 8, 9, 10. The premature layers of DoLa-static are selected by the performance on validation sets. If it is in a two-fold validation setting, we report both of the selected layers in the tables (Val Selected Layer).

We can observe that for TruthfulQA and FACTOR, DoLa-static is slightly better than DoLa in most of the cases. However, for StrategyQA and GSM8K, DoLa can consistently outperform DoLa-static. Considering that DoLa is more robust and generalizable, only requiring a very small hyperparameter search space, we use DoLa as our main proposed method, instead of DoLa-static.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **Model** & **Bucket** & **Layer Range** \\ \hline \multirow{8}{*}{TruthfulQA} & LLaMA-7B & 2nd (out of 2) & [16, 32] \\  & LLaMA-13B & 2nd (out of 2) & [20, 40] \\  & LLaMA-33B & 3rd (out of 3) & [40, 60] \\  & LLaMA-65B & 4th (out of 4) & [60, 80] \\  & MPI-7B & 4th (out of 4) & [24, 32] \\ \hline \multirow{8}{*}{FACTOR \& GSM8K} & LLaMA-7B & 1st (out of 2) & [0, 16] \\  & LLaMA-13B & 1st (out of 2) & [0, 20] \\ \cline{1-1}  & LLaMA-33B & 1st (out of 3) & [0, 20] \\ \cline{1-1}  & LLaMA-65B & 1st (out of 4) & [0, 20] \\ \cline{1-1}  & MPT-7B & 1st (out of 4) & [0, 8] \\ \hline \hline \end{tabular}
\end{table}
Table 7: Best Bucket Selected by Validation Set

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **Val Selected Layer** & **MC1** & **MC2** & **MC3** \\ \hline LLaMa-7B & - & 25.6 & 40.6 & 19.2 \\ + DoLa-static & 30/30 & **34.5** & **68.3** & **40.0** \\ + DoLa & [16, 32) & 32.2 & 63.8 & 32.1 \\ \hline LLaMa-13B & - & 28.3 & 43.3 & 20.8 \\ + DoLa-static & 38/38 & **33.0** & **66.9** & **38.4** \\ + DoLa & [20, 40) & 28.9 & 64.9 & 34.8 \\ \hline LLaMa-33B & - & 31.7 & 49.5 & 24.2 \\ + DoLa-static & 50/38 & 27.9 & 61.9 & 33.7 \\ + DoLa & [40, 60) & **30.5** & **62.3** & **34.0** \\ \hline LLaMa-65B & - & 30.8 & 46.9 & 22.7 \\ + DoLa-static & 36/72 &

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\ \hline
**Question** & What if Christopher Columbus had not discovered the Americas? \\ \hline
**Answer** & If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would not have been able to establish the American colonies, which eventually became the United States. 1. A different continent: If Christopher Columbus had not discovered the Americas, then it is likely that European would have been forced to find a different continent to settle, which would have changed the course of history. 2. A different America: If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would have settled the Americas at a different point in history, which would have changed the course of history. 3. No America: If Christopher Columbus had not discovered the Americas, then it is likely that Europeons would never have settled the Americas, which would have changed the course of history. \\ \hline
**Score** & 7 & 9 \\ \hline
**Review** & Assistant 1’s answer provided three possible scenarios, but they were repetitive and lacked depth. \\ \hline \hline \end{tabular}
\end{table}
Table 12: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\ \hline
**Question** & How many atoms are in a grain of salt? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. \\ \hline
**Answer** & A grain of salt is about 250 micrometers in diameter. A micrometer is 1 millionth of a meter. One millionth of a meter is \(10^{-6}\) meters. There are \(10^{23}\) atoms in one cubic meter of air. Since a grain of salt has a volume of \(10^{-12}\) cubic meters, there are \(10^{11}\) atoms in a grain of salt. & One grain of salt contains \(7.025\times 10^{25}\) atoms. This is calculated by dividing the mass of one grain of salt (0.018 g) by the atomic weight of sodium chloride (39.94 g/mol), giving approximately \(4.43\times 10^{20}\) molecules of NaCl per grain of salt. Since each molecule of NaCl has two sodium atoms and one chlorine atom, this means there are about 8.86 \(\times\)\(10^{20}\) atoms of sodium and \(8.86\times\)\(10^{20}\) atoms of chlorine per grain of salt. Therefore, one grain of salt contains \(1.77\times 10^{26}\) total atoms, or \(7.025\times 10^{25}\) atoms. \\ \hline
**Score** & 4 & 8 \\ \hline
**Review** & Assistant 1’s answer is incorrect and lacks clarity in its explanation. The assistant made an incorrect assumption about the number of atoms in a cubic meter of air and applied it to the grain of salt, which is not relevant. & Assistant 2’s answer is more accurate, detailed, and relevant. It provides a step-by-step explanation of the calculation, considering the mass of a grain of salt, the atomic weight of sodium chloride, and the number of sodium and chlorine atoms in each molecule. The final answer provided is reasonable and well-explained.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\ \hline
**Question** & What if Christopher Columbus had not discovered the Americas, then Europeons would have continued to trade with the East Indies via the Silk Road and the spice trade. The Portuguese would have maintained their monopoly over the spice trade. In 1487, Bartolomeu Dias became the first European to round the Cape of Good Hope. He established a direct sea route from Portugal to India. Vasco da Gama salied around Africa to India in 1498. This made Portugal the dominant maritime power in the Indian Ocean. By 1502, the Portuguese were trading in China. They also established trading posts in India, Malaysia, and Indonesia. If Christopher Columbus had not discovered the Americas, then the Spanish would not have been able to establish a foothold in the New World, and the Aztec and Inca empesures would not have fallen. This would have left the way open for the Portuguese to expand into Central and South America... (skip text that are too long) \\ \hline
**Score** & 7 & 9 \\ \hline
**Review** & Assistant 1’s answer provided three possible scenarios, but they were repetitive and lacked depth. & Assistant 2’s answer, on the other hand, was more detailed and provided historical context, such as the Portuguese exploration and the Columbian Exchange. Assistant 2’s answer was more helpful, relevant, and accurate, which is why it received a higher score. \\ \hline \hline \end{tabular}
\end{table}
Table 13: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.

[MISSING_PAGE_FAIL:21]

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Baseline (LLAMA-33B)** & **DoLa (LLAMA-33B)** \\ \hline
**Question** & As a space colonist on Mars, describe your daily life and the challenges you face living on another planet. \\ \hline  & & Assistant 2: On Mars, the day is 24 hours, 39 minutes, and 35 seconds long, compared to 23 hours, 56 minutes, and 4.09 seconds on Earth. So, it’s about 2.7 percent longer than our days on Earth. This is due to the difference in rotational speed between Mars and Earth. Also, because Mars has an elliptical orbit around the Sun, its day length varies by about 20 minutes throughout the year. On Mars, a year is equal to 687 Earth days. So, a Martian year is almost twice as long as an Earth year. The average surface temperature on Mars is -81 degrees Fahrenheit (-63 degrees Celsius), making it too cold for humans to live there without protection from the elements. In addition, the atmospheric pressure is only 0.006 times that of Earth’s atmosphere, so any humans who visit Mars must wear pressurized suits or live in pressurized habitats. There’s also less gravity on Mars. Mars’s gravitational acceleration at the surface is 0.38 of that of Earth’s, meaning that an object weighing 100 pounds on Earth would weigh 38 pounds on Mars. So, humans could jump higher and do more work with less effort on Mars. However, this would also mean that the bones and muscles of a person living on Mars for a long period of time would get weaker. There is no liquid water on Mars, but there is ice in some places. Water is necessary for survival, and ice is necessary to make water, so finding and accessing ice on Mars would be essential for any human colony. Mars also doesn’t have a magnetic field, which means that cosmic radiation from space reaches the surface of Mars unimpeded, increasing the risk of cancer for anyone living there. In addition, Mars doesn’t have a protective ozone layer like Earth does, so ultraviolet radiation from the Sun would reach the surface of Mars and harm any humans who didn’t take precautions to protect themselves. Mars is known as the Red Planet due to its reddish appearance. This is caused by iron oxide (rust) in the soil of Mars. It’s also called the Red Planet because the Roman god of water was named Mars. Mars is the fourth planet from the Sun and is named after the Roman god of war. Its symbol is a circle with an arrow through it pointing to the upper right. Mars has two moons: Phobos and Deimos. Phobos is about 14 miles (22 kilometers) in diameter, while Deimos is about 8 miles (13 kilometers) in diameter. Phobos orbits Mars faster than the planet itself rotates, so it rises in the west and sets in the east, just as the Sun and the Moon do on Earth... (ignore the following text due to the limited space) & 6 Assistant 2’s answer, while informative, focused more on general facts about Mars and its features, rather than addressing the daily life and challenges of a space colonist. The information provided by Assistant 2 was accurate but not as relevant to the question as Assistant 1’s response. \\ \hline \hline \end{tabular}
\end{table}
Table 15: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.

\begin{table}
\begin{tabular}{l|l|l} \hline \hline
**Method** & **Baseline (LLaMA-33B)** & **DoLa (LLaMA-33B)** \\ \hline
**Question** & How many balloons would it take to lift a house like in the movie “Up”? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step. \\ \hline \hline \multirow{4}{*}{**Answer**} & \multirow{4}{*}{According to the movie, the house weighed 4,400 pounds, or 2 tons. This is equivalent to 18,144.14 Newtons. The amount of lift provided by a helium balloon is given by the buoyant force. The buoyant force is equal to the weight of the volume of fluid displaced by the balloon. This is given by the equation: L = -\(\rho\)gV L = buoyant force (Neurons) \(\rho\) = density of the fluid (Neurons/meter\({}^{2}\)) \(\beta\) = accelerative due to gravity (meters/second\({}^{2}\)) \(\nabla\) = volume of the balloon (meters\({}^{2}\)) \(\nabla\) = density of the problem is about 0.1786 kilograms per cubic meter. The acceleration due to gravity is 9.81 meters per second squared. This means that the buoyant force is equal to: L = -\(\rho\)gV = -(0.1786 kg/m\({}^{3}\))(9.81 m/s\({}^{2}\)) \(\nabla\) = -1.7132496 V This means that a balloon with a volume of 1 cubic meter will have a buoyant of 1.7132496 Neutons. This means that 10,588.86 balloons would be needed to lift 4,400 pounds, or 2 tons. \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \hline \hline \end{tabular} 
\begin{tabular}{l|l} \hline
**Score** & 7 & 5 \\ \hline \hline \multirow{4}{*}{**Review**} & Assistant 1 provided a more concise and clear explanation, using the buoyant force equation and the weight of the house from the movie “Up” to calculate the number of balloons needed. However, the final number of balloons was slightly off. \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \cline{2-3}  & & \\ \hline \hline \end{tabular} 
\begin{tabular}{l|l} \hline
**Score** & 7 & 5 \\ \hline \hline \end{tabular} 
\begin{tabular}{l|l} \hline
**Score** & Assistant 1 provided a more concise and clear explanation, using the buoyant force equation and the weight of the house from the movie “Up” to calculate the number of balloons needed. However, the final number of balloons was slightly off. \\ \hline \hline \end{tabular}
\end{table}
Table 16: Qualitative Study for LLaMA-33B and DoLa with GPT-4 judgement.